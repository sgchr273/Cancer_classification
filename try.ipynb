{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/home/sgchr/.local/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-11-11 13:29:32.686816: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-11-11 13:29:32.731361: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-11 13:29:33.370253: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8812 images belonging to 2 classes.\n",
      "Found 712 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import cv2\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from torchvision import transforms\n",
    "from dataset import get_handler, get_dataset\n",
    "from dataset import get_dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=10):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "        self.embDim = 128 * block.expansion\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 128, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(2048 * block.expansion, num_classes, bias=False)\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 4)\n",
    "        emb = out.view(out.size(0), -1)\n",
    "        out = self.linear(emb)\n",
    "        return out, emb\n",
    "    def get_embedding_dim(self):\n",
    "        return self.embDim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18(num_classes=2):\n",
    "    return ResNet(BasicBlock, [2,2,2,2], num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18(num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.randn(100, 3,128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path_benign = '/usr/local/home/sgchr/Documents/Cancer_classification/BreaKHis_v1/Cancer_train/benign'\n",
    "image_files_benign = [os.path.join(folder_path_benign, filename) for filename in os.listdir(folder_path_benign)]\n",
    "\n",
    "folder_path_mal = '/usr/local/home/sgchr/Documents/Cancer_classification/BreaKHis_v1/Cancer_train/malignant'\n",
    "image_files_mal = [os.path.join(folder_path_mal, filename) for filename in os.listdir(folder_path_mal)]\n",
    "\n",
    "from PIL import Image\n",
    "X_tr = []\n",
    "common_size = (128, 128)\n",
    "\n",
    "for image_file in image_files_benign:\n",
    "    img = cv2.imread(image_file)\n",
    "    if img is not None:\n",
    "        # Resize the image to the common size\n",
    "        img = cv2.resize(img, common_size)\n",
    "        # Image.fromarray((img/255).astype(np.uint8))\n",
    "        X_tr.append(img)\n",
    "    else:\n",
    "        print(f\"Error loading image: {image_file}\")\n",
    "\n",
    "\n",
    "for image_file in image_files_mal:\n",
    "    img = cv2.imread(image_file)\n",
    "    if img is not None:\n",
    "        # Resize the image to the common size\n",
    "        img = cv2.resize(img, common_size)\n",
    "        X_tr.append(img)\n",
    "    else:\n",
    "        print(f\"Error loading image: {image_file}\")\n",
    "X_tr = np.array(X_tr)\n",
    "X_tr = torch.tensor(X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.uint8\n"
     ]
    }
   ],
   "source": [
    "print(X_tr.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the number of samples in each part\n",
    "total_samples = X_tr.shape[0]\n",
    "samples_per_part = total_samples // 6\n",
    "\n",
    "# Divide the tensor into three parts\n",
    "part1 = X_tr[:samples_per_part]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_te, Y_tr, Y_te = get_dataset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = torch.tensor(X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = np.array(X_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CancerModel(nn.Module):\n",
    "    def __init__(self, num_classes = 2):\n",
    "        super(CancerModel, self).__init__()\n",
    "\n",
    "        self.embDim = 128\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        \n",
    "        self.conv3 = nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout1 = nn.Dropout(0.4)\n",
    "        \n",
    "        self.conv4 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(12544, 256)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.relu6 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(128, 128)\n",
    "        self.fc_multiclass = nn.Linear(128, num_classes)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool1(self.relu1(self.conv1(x)))\n",
    "        x = self.maxpool2(self.relu2(self.conv2(x)))\n",
    "        x = self.maxpool3(self.relu3(self.conv3(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = self.maxpool4(self.relu4(self.conv4(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu5(self.fc1(x))\n",
    "        x = self.relu6(self.fc2(x))\n",
    "        x = self.sigmoid(self.fc3(x))\n",
    "        multiclass_output = self.fc_multiclass(x)\n",
    "        return  x   #return multiclass_ouput as well\n",
    "    \n",
    "    def get_embedding_dim(self):\n",
    "        return self.embDim\n",
    "cancer_model = CancerModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr =np.transpose(X_tr, (0, 3, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, inputs, labels, transform=None):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        input_data = self.inputs[index]\n",
    "        label = self.labels[index]\n",
    "\n",
    "        if self.transform:\n",
    "            input_data = self.transform(input_data)\n",
    "\n",
    "        return input_data, label\n",
    "    \n",
    "transforms_list = [transforms.RandomRotation(20),\n",
    "                        transforms.RandomHorizontalFlip(),\n",
    "                        transforms.RandomVerticalFlip(),\n",
    "                        transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "                        # transforms.RandomResizedCrop(128),  # Assuming you want to resize images to 128x128\n",
    "                        transforms.RandomAffine(degrees=0, translate=(0.2, 0.2), shear=(0.2, 0.2)),\n",
    "                        # transforms.Lambda(lambda x: custom_divide(x)),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])]\n",
    "transform = transforms.Compose(transforms_list)  # ImageNet mean and std\n",
    "                    \n",
    "\n",
    "# Create custom datasets\n",
    "train_dataset = CustomDataset(inputs=X_tr, labels=Y_tr, transform=transform)\n",
    "test_dataset = CustomDataset(inputs=X_te, labels=Y_te, transform=transform)\n",
    "\n",
    "# Create DataLoader instances\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/usr/local/home/sgchr/Documents/Cancer_classification/try.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Br11ast8hv.managed.mst.edu/usr/local/home/sgchr/Documents/Cancer_classification/try.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m correct_predictions \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Br11ast8hv.managed.mst.edu/usr/local/home/sgchr/Documents/Cancer_classification/try.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=9'>10</a>\u001b[0m total_samples \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Br11ast8hv.managed.mst.edu/usr/local/home/sgchr/Documents/Cancer_classification/try.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m inputs, labels \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Br11ast8hv.managed.mst.edu/usr/local/home/sgchr/Documents/Cancer_classification/try.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m# Step 4: Forward pass\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Br11ast8hv.managed.mst.edu/usr/local/home/sgchr/Documents/Cancer_classification/try.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m     outputs \u001b[39m=\u001b[39m cancer_model(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Br11ast8hv.managed.mst.edu/usr/local/home/sgchr/Documents/Cancer_classification/try.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m# Step 5: Compute loss\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    625\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    626\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    627\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 628\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    629\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    631\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    632\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    669\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    670\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 671\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    672\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    673\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/utils/data/_utils/fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     56\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset\u001b[39m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     57\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 58\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     59\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/usr/local/home/sgchr/Documents/Cancer_classification/try.ipynb Cell 17\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Br11ast8hv.managed.mst.edu/usr/local/home/sgchr/Documents/Cancer_classification/try.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=11'>12</a>\u001b[0m label \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlabels[index]\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Br11ast8hv.managed.mst.edu/usr/local/home/sgchr/Documents/Cancer_classification/try.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransform:\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Br11ast8hv.managed.mst.edu/usr/local/home/sgchr/Documents/Cancer_classification/try.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=14'>15</a>\u001b[0m     input_data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform(input_data)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Br11ast8hv.managed.mst.edu/usr/local/home/sgchr/Documents/Cancer_classification/try.ipynb#X64sdnNjb2RlLXJlbW90ZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39mreturn\u001b[39;00m input_data, label\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, img):\n\u001b[1;32m     94\u001b[0m     \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtransforms:\n\u001b[0;32m---> 95\u001b[0m         img \u001b[39m=\u001b[39m t(img)\n\u001b[1;32m     96\u001b[0m     \u001b[39mreturn\u001b[39;00m img\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/transforms.py:135\u001b[0m, in \u001b[0;36mToTensor.__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, pic):\n\u001b[1;32m    128\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    129\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m    130\u001b[0m \u001b[39m        pic (PIL Image or numpy.ndarray): Image to be converted to tensor.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[39m        Tensor: Converted image.\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 135\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mto_tensor(pic)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torchvision/transforms/functional.py:137\u001b[0m, in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m    135\u001b[0m     _log_api_usage_once(to_tensor)\n\u001b[1;32m    136\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (F_pil\u001b[39m.\u001b[39m_is_pil_image(pic) \u001b[39mor\u001b[39;00m _is_numpy(pic)):\n\u001b[0;32m--> 137\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpic should be PIL Image or ndarray. Got \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mtype\u001b[39m(pic)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[39mif\u001b[39;00m _is_numpy(pic) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_numpy_image(pic):\n\u001b[1;32m    140\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpic should be 2/3 dimensional. Got \u001b[39m\u001b[39m{\u001b[39;00mpic\u001b[39m.\u001b[39mndim\u001b[39m}\u001b[39;00m\u001b[39m dimensions.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: pic should be PIL Image or ndarray. Got <class 'torch.Tensor'>"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cancer_model.parameters(), lr=0.001)\n",
    "\n",
    "# Step 3: Training loop\n",
    "num_epochs = 5\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        # Step 4: Forward pass\n",
    "        outputs = cancer_model(inputs)\n",
    "        \n",
    "        # Step 5: Compute loss\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Step 6: Backward pass and optimization\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Calculate accuracy\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct_predictions += (predicted == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    # Print statistics\n",
    "    print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader)}, Accuracy: {correct_predictions / total_samples}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.randn(100, 3, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_1 = cancer_model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.uint8\n"
     ]
    }
   ],
   "source": [
    "print(X_tr.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "for param in cancer_model.parameters():\n",
    "    print(param.dtype)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr = X_tr.to(dtype=torch.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "reshaped_Xtr = X_tr.permute(0, 3, 1, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = cancer_model.forward(input_tensor)\n",
    "output = cancer_model(reshaped_Xtr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "handler = get_handler('BreaKHis')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "type"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "args =  {'n_epoch': 20, 'transform':transforms.Compose([\n",
    "                        # transforms.RandomRotation(20),\n",
    "                        # transforms.RandomHorizontalFlip(),\n",
    "                        # transforms.RandomVerticalFlip(),\n",
    "                        # transforms.ColorJitter(brightness=0.1, contrast=0.1, saturation=0.1, hue=0.1),\n",
    "                        # transforms.RandomResizedCrop(128),  # Assuming you want to resize images to 128x128\n",
    "                        # transforms.RandomAffine(degrees=0, translate=(0.2, 0.2), shear=(0.2, 0.2)),\n",
    "                        # # transforms.Lambda(lambda x: custom_divide(x)),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet mean and std\n",
    "                    ]),\n",
    "                    'loader_tr_args':{'batch_size': 64, 'num_workers': 1},\n",
    "                    'loader_te_args':{'batch_size': 100, 'num_workers': 1},\n",
    "                    'optimizer_args':{'lr': 0.001, 'momentum': 0.5},\n",
    "                    'transformTest' : transforms.Compose([\n",
    "                        transforms.Resize(128),\n",
    "                        transforms.CenterCrop(128),\n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "                    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tr, X_te, Y_tr, Y_te = get_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "print(X_te.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "float64\n"
     ]
    }
   ],
   "source": [
    "print(X_tr.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_tr = Y_tr[torch.randperm(len(Y_tr))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(4406)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(Y_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "one_fourth_index = len(X_tr) // 10\n",
    "\n",
    "one_fourth_index_y = len(Y_tr) // 10\n",
    "\n",
    "# Slice the data to get one fourth of the dataset\n",
    "one_fourth_data = X_tr[:one_fourth_index]\n",
    "one_fourth_data_y = Y_tr[:one_fourth_index_y]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(466)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(one_fourth_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict(net,X, Y):\n",
    "    if type(X) is np.ndarray:\n",
    "        loader_te = DataLoader(handler(X, Y, transform=args['transformTest']),\n",
    "                        shuffle=False, **args['loader_te_args'])\n",
    "    else: \n",
    "        loader_te = DataLoader(handler(X.numpy(), Y, transform=args['transformTest']),\n",
    "                        shuffle=False, **args['loader_te_args'])\n",
    "\n",
    "    net.eval()\n",
    "    P = torch.zeros(len(Y)).long()\n",
    "    with torch.no_grad():\n",
    "        for x, y, idxs in loader_te:\n",
    "            x, y = Variable(x), Variable(y)\n",
    "            out, e1 = net(x)\n",
    "            pred = out.max(1)[1]\n",
    "            P[idxs] = pred.data.cpu()\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = predict(cancer_model,one_fourth_data, one_fourth_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_zero = torch.any(P==0).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(881)"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_acc(net,X,Y):\n",
    "    P = predict(net,X, Y)\n",
    "    accur = 1.0 * (Y == P).sum().item() / len(Y)\n",
    "    return accur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = check_acc(cancer_model,one_fourth_data, one_fourth_data_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != self.expansion*planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, self.expansion*planes, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(self.expansion*planes)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)\n",
    "        out = F.relu(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    def __init__(self, block, num_blocks, num_classes=2):\n",
    "        super(ResNet, self).__init__()\n",
    "        self.in_planes = 16\n",
    "        self.embDim = 128 * block.expansion\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.layer1 = self._make_layer(block, 16, num_blocks[0], stride=1)\n",
    "        self.layer2 = self._make_layer(block, 32, num_blocks[1], stride=2)\n",
    "        self.layer3 = self._make_layer(block, 64, num_blocks[2], stride=2)\n",
    "        self.layer4 = self._make_layer(block, 128, num_blocks[3], stride=2)\n",
    "        self.linear = nn.Linear(128 * block.expansion, num_classes, bias=False)\n",
    "    def _make_layer(self, block, planes, num_blocks, stride):\n",
    "        strides = [stride] + [1]*(num_blocks-1)\n",
    "        layers = []\n",
    "        for stride in strides:\n",
    "            layers.append(block(self.in_planes, planes, stride))\n",
    "            self.in_planes = planes * block.expansion\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = F.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.layer1(out)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = F.avg_pool2d(out, 2)\n",
    "        emb = out.view(out.size(0), -1)\n",
    "        out = self.linear(emb)\n",
    "        return out, emb\n",
    "    def get_embedding_dim(self):\n",
    "        return self.embDim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ResNet18(num_classes):\n",
    "    return ResNet(BasicBlock, [2,2,2,2], num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet18(num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tensor = torch.randn(100, 3, 128, 128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (100x8192 and 128x2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/usr/local/home/sgchr/Documents/Cancer_classification/try.ipynb Cell 9\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Br11ast8hv.managed.mst.edu/usr/local/home/sgchr/Documents/Cancer_classification/try.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m output \u001b[39m=\u001b[39m model(input_tensor)\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/usr/local/home/sgchr/Documents/Cancer_classification/try.ipynb Cell 9\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Br11ast8hv.managed.mst.edu/usr/local/home/sgchr/Documents/Cancer_classification/try.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=26'>27</a>\u001b[0m out \u001b[39m=\u001b[39m F\u001b[39m.\u001b[39mavg_pool2d(out, \u001b[39m2\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Br11ast8hv.managed.mst.edu/usr/local/home/sgchr/Documents/Cancer_classification/try.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m emb \u001b[39m=\u001b[39m out\u001b[39m.\u001b[39mview(out\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2Br11ast8hv.managed.mst.edu/usr/local/home/sgchr/Documents/Cancer_classification/try.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlinear(emb)\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2Br11ast8hv.managed.mst.edu/usr/local/home/sgchr/Documents/Cancer_classification/try.ipynb#X10sdnNjb2RlLXJlbW90ZQ%3D%3D?line=29'>30</a>\u001b[0m \u001b[39mreturn\u001b[39;00m out, emb\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (100x8192 and 128x2)"
     ]
    }
   ],
   "source": [
    "output = model(input_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example data\n",
    "labels = ['Precision', 'Recall', 'F1 Score']\n",
    "scores = [0.85, 0.78, 0.81]  # Replace with your actual scores\n",
    "\n",
    "# Plotting\n",
    "fig, ax = plt.subplots()\n",
    "bars = ax.bar(labels, scores, color=['blue', 'green', 'orange'])\n",
    "\n",
    "# Adding data labels\n",
    "for bar in bars:\n",
    "    yval = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, yval, round(yval, 2), ha='center', va='bottom')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision, Recall, and F1 Score')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cancer_class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
